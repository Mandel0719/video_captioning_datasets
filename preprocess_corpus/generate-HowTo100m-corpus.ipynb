{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate corpus and gruond-truth references of released videos\n",
    "\n",
    "### Corpus file contents\n",
    "0. train_data: captions and idxs of training videos in format [corpus_widxs, vidxs, corpus_pidxs], where:\n",
    "    - corpus_widxs is a list of lists with the index of words in the vocabulary\n",
    "    - vidxs is a list of indexes of video features in the features file\n",
    "    - corpus_pidxs is a list of lists with the index of POS tags in the POS tagging vocabulary\n",
    "1. val_data: same format of train_data.\n",
    "2. test_data: same format of train_data.\n",
    "3. vocabulary: in format {'word': count}.\n",
    "4. idx2word: is the vocabulary in format {idx: 'word'}.\n",
    "5. word_embeddings: are the vectors of each word. The i-th row is the word vector of the i-th word in the vocabulary.\n",
    "6. idx2pos: is the vocabulary of POS tagging in format {idx: 'POSTAG'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate split for training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('../../../data/HowTo100M/caption.json') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count of training pairs:  139668840\n"
     ]
    }
   ],
   "source": [
    "train_vidxs, train_corpus = [], []\n",
    "for vid, subtitles in data.items():\n",
    "    train_vidxs.append(vid)\n",
    "    for subtitle in subtitles['text']:\n",
    "        train_vidxs.append(vid)\n",
    "        train_corpus.append(str(subtitle))\n",
    "        \n",
    "print('count of training pairs: ', len(train_vidxs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get pretrained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "wordvectors = {}\n",
    "# with open('./glove.42B.300d.txt') as f:\n",
    "with open('./glove.6B.300d.txt') as f:\n",
    "    for line in f:\n",
    "        s = line.strip().split(' ')\n",
    "        if len(s) == 301:\n",
    "            wordvectors[s[0]] = np.array(s[1:], dtype=float)\n",
    "    print(len(wordvectors))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine the vocabulary from train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jeperez/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parallel processing\n",
      "merging partials\n",
      "0 partial merged\n",
      "1 partial merged\n",
      "2 partial merged\n",
      "3 partial merged\n",
      "4 partial merged\n",
      "5 partial merged\n",
      "6 partial merged\n",
      "7 partial merged\n",
      "8 partial merged\n",
      "9 partial merged\n",
      "10 partial merged\n",
      "11 partial merged\n",
      "12 partial merged\n",
      "13 partial merged\n",
      "14 partial merged\n",
      "15 partial merged\n",
      "16 partial merged\n",
      "17 partial merged\n",
      "18 partial merged\n",
      "19 partial merged\n",
      "20 partial merged\n",
      "21 partial merged\n",
      "22 partial merged\n",
      "23 partial merged\n",
      "24 partial merged\n",
      "25 partial merged\n",
      "26 partial merged\n",
      "27 partial merged\n",
      "28 partial merged\n",
      "29 partial merged\n",
      "30 partial merged\n",
      "Avg. count of words per caption: 4.160451942440858\n",
      "Count of unique words:  593238\n",
      "count of missing words:  375877\n",
      "217361 217363 217363\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import multiprocessing as mp\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "def determine_vocab(train_corpus):\n",
    "    vocab, total_len = {}, 0\n",
    "    for cap in train_corpus:\n",
    "        tokens = nltk.word_tokenize(cap.lower())\n",
    "        total_len += len(tokens)\n",
    "        for w in tokens:\n",
    "            try:\n",
    "                vocab[w] += 1\n",
    "            except:\n",
    "                vocab[w] = 1\n",
    "    return vocab, total_len\n",
    "\n",
    "def merge_vocabs(partial_results):\n",
    "    vocab, total_len  = {}, 0\n",
    "    for i, (vocab_partial, total_len_partial) in enumerate(partial_results):\n",
    "        total_len += total_len_partial\n",
    "        for word, count in vocab_partial.items():\n",
    "            try: \n",
    "                vocab[word] += count\n",
    "            except:\n",
    "                vocab[word] = count\n",
    "                \n",
    "        print(f'{i} partial merged')\n",
    "    return vocab, total_len    \n",
    "\n",
    "num_threads = mp.cpu_count()-2 #30\n",
    "step = len(train_corpus) // num_threads \n",
    "args = [train_corpus[idx:idx+step] for idx in range(0, len(train_corpus), step)]\n",
    "\n",
    "print('parallel processing')\n",
    "pool = Pool(num_threads)\n",
    "partial_results = pool.map(determine_vocab, args)\n",
    "\n",
    "print('merging partials')\n",
    "vocab, total_len = merge_vocabs(partial_results)\n",
    "\n",
    "print('Avg. count of words per caption:', total_len/len(train_corpus))\n",
    "print('Count of unique words: ', len(vocab))\n",
    "\n",
    "to_del = []\n",
    "for w in vocab.keys():\n",
    "    if not w in wordvectors:\n",
    "        to_del.append(w)\n",
    "#         print('missing word: {}'.format(w))\n",
    "\n",
    "print('count of missing words: ', len(to_del))\n",
    "        \n",
    "for w in to_del:\n",
    "    del vocab[w]\n",
    "        \n",
    "idx2word = {idx: word for idx, word in enumerate(['<eos>', '<unk>'] + list(vocab.keys()))}\n",
    "word2idx = {word: idx for idx, word in enumerate(['<eos>', '<unk>'] + list(vocab.keys()))}\n",
    "EOS, UNK = 0, 1\n",
    "\n",
    "print(len(vocab), len(idx2word), len(word2idx))\n",
    "\n",
    "word_embeddings = np.zeros((len(idx2word), 300))\n",
    "for idx, word in idx2word.items():\n",
    "    if idx == EOS:\n",
    "        word_embeddings[idx] = wordvectors['eos']\n",
    "    elif idx == UNK:\n",
    "        word_embeddings[idx] = wordvectors['unk']\n",
    "    else:\n",
    "        word_embeddings[idx] = wordvectors[word] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine POS-tagging vocabulary from train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/jeperez/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parallel processing\n",
      "merging partials\n",
      "0 partial merged\n",
      "1 partial merged\n",
      "2 partial merged\n",
      "3 partial merged\n",
      "4 partial merged\n",
      "5 partial merged\n",
      "6 partial merged\n",
      "7 partial merged\n",
      "8 partial merged\n",
      "9 partial merged\n",
      "10 partial merged\n",
      "11 partial merged\n",
      "12 partial merged\n",
      "13 partial merged\n",
      "14 partial merged\n",
      "15 partial merged\n",
      "16 partial merged\n",
      "17 partial merged\n",
      "18 partial merged\n",
      "19 partial merged\n",
      "20 partial merged\n",
      "21 partial merged\n",
      "22 partial merged\n",
      "23 partial merged\n",
      "24 partial merged\n",
      "25 partial merged\n",
      "26 partial merged\n",
      "27 partial merged\n",
      "28 partial merged\n",
      "29 partial merged\n",
      "30 partial merged\n",
      "Unique words per tag:\n",
      " NNS:\t136311\n",
      " VBP:\t131078\n",
      " NN:\t422846\n",
      " VBN:\t23227\n",
      " RB:\t72254\n",
      " VBD:\t56598\n",
      " JJ:\t216737\n",
      " VBG:\t13493\n",
      " VB:\t62911\n",
      " IN:\t28770\n",
      " POS:\t3100\n",
      " VBZ:\t57678\n",
      " WP:\t886\n",
      " RP:\t8588\n",
      " CD:\t40352\n",
      " RBR:\t10734\n",
      " PRP:\t5495\n",
      " MD:\t4046\n",
      " JJR:\t7427\n",
      " UH:\t54\n",
      " TO:\t69\n",
      " JJS:\t1984\n",
      " FW:\t16056\n",
      " DT:\t3841\n",
      " NNP:\t17979\n",
      " CC:\t8943\n",
      " WRB:\t1997\n",
      " WDT:\t537\n",
      " EX:\t1373\n",
      " RBS:\t4463\n",
      " LS:\t60\n",
      " '':\t1738\n",
      " SYM:\t48\n",
      " PRP$:\t431\n",
      " $:\t4896\n",
      " PDT:\t50\n",
      " ``:\t1\n",
      " NNPS:\t183\n",
      " WP$:\t1\n",
      "41\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import multiprocessing as mp\n",
    "from multiprocessing import Pool\n",
    "from functools import partial\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "def count_words_per_tag(train_corpus, tagset=None):\n",
    "    upos_vocab,upos_unique_words  = {},{}\n",
    "    for cap in train_corpus:\n",
    "        for tag in nltk.pos_tag(nltk.word_tokenize(cap.lower()), tagset=tagset):\n",
    "            try:\n",
    "                upos_vocab[tag[1]] += 1\n",
    "                try: \n",
    "                    upos_unique_words[tag[1]][tag[0]] += 1\n",
    "                except:\n",
    "                    upos_unique_words[tag[1]][tag[0]] = 1\n",
    "            except:\n",
    "                upos_vocab[tag[1]] = 1\n",
    "                upos_unique_words[tag[1]] = {tag[0]: 1}\n",
    "    return upos_vocab, upos_unique_words\n",
    "\n",
    "def merge_tags(partial_results):\n",
    "    tags_vocab, tag_unique_words  = {}, {}\n",
    "    for i, (tags_vocab_partial, tag_unique_words_partial) in enumerate(partial_results):\n",
    "        for tag, count in tags_vocab_partial.items():\n",
    "            try: \n",
    "                tags_vocab[tag] += count\n",
    "            except:\n",
    "                tags_vocab[tag] = count\n",
    "\n",
    "        for tag, unique_words in tag_unique_words_partial.items():\n",
    "            for word, count in unique_words.items():\n",
    "                try: \n",
    "                    tag_unique_words[tag][word] += count\n",
    "                except:\n",
    "                    try:\n",
    "                        tag_unique_words[tag][word] = count\n",
    "                    except:\n",
    "                        tag_unique_words[tag] = {word: count}\n",
    "        print(f'{i} partial merged')\n",
    "    return tags_vocab, tag_unique_words\n",
    "\n",
    "num_threads = mp.cpu_count()-2  # 30\n",
    "step = len(train_corpus) // num_threads\n",
    "args = [train_corpus[idx:idx+step] for idx in range(0, len(train_corpus), step)]\n",
    "\n",
    "print('parallel processing')\n",
    "pool = Pool(num_threads)\n",
    "partial_results = pool.map(partial(count_words_per_tag, tagset=None), args)\n",
    "\n",
    "print('merging partials')\n",
    "pos_vocab, pos_unique_words = merge_tags(partial_results)\n",
    "\n",
    "print('Unique words per tag:')\n",
    "print('\\n'.join([f' {k}:\\t{len(words)}' for k, words in pos_unique_words.items()]))\n",
    "            \n",
    "idx2pos = {idx: tag for idx, tag in enumerate(['eos', 'unk'] + list(pos_vocab.keys()))}\n",
    "pos2idx = {tag: idx for idx, tag in enumerate(['eos', 'unk'] + list(pos_vocab.keys()))}\n",
    "EOS, UNK = 0, 1\n",
    "print(len(idx2pos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determine Universal POS-tagging from train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /home/jeperez/nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parallel processing\n",
      "merging partials\n",
      "0 partial merged\n",
      "1 partial merged\n",
      "2 partial merged\n",
      "3 partial merged\n",
      "4 partial merged\n",
      "5 partial merged\n",
      "6 partial merged\n",
      "7 partial merged\n",
      "8 partial merged\n",
      "9 partial merged\n",
      "10 partial merged\n",
      "11 partial merged\n",
      "12 partial merged\n",
      "13 partial merged\n",
      "14 partial merged\n",
      "15 partial merged\n",
      "16 partial merged\n",
      "17 partial merged\n",
      "18 partial merged\n",
      "19 partial merged\n",
      "20 partial merged\n",
      "21 partial merged\n",
      "22 partial merged\n",
      "23 partial merged\n",
      "24 partial merged\n",
      "25 partial merged\n",
      "26 partial merged\n",
      "27 partial merged\n",
      "28 partial merged\n",
      "29 partial merged\n",
      "30 partial merged\n",
      "Unique words per universal tag:\n",
      " NOUN:\t491488\n",
      " VERB:\t198859\n",
      " ADV:\t76535\n",
      " ADJ:\t219719\n",
      " ADP:\t28770\n",
      " PRT:\t11311\n",
      " PRON:\t6477\n",
      " NUM:\t40352\n",
      " X:\t16121\n",
      " DET:\t5473\n",
      " CONJ:\t8943\n",
      " .:\t6072\n",
      "14\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import multiprocessing as mp\n",
    "from multiprocessing import Pool\n",
    "\n",
    "import nltk\n",
    "nltk.download('universal_tagset')\n",
    "\n",
    "num_threads = mp.cpu_count()-2  # 30\n",
    "step = len(train_corpus) // num_threads\n",
    "args = [train_corpus[idx:idx+step] for idx in range(0, len(train_corpus), step)]\n",
    "\n",
    "print('parallel processing')\n",
    "pool = Pool(num_threads)\n",
    "partial_results = pool.map(partial(count_words_per_tag, tagset='universal'), args)\n",
    "\n",
    "print('merging partials')\n",
    "upos_vocab, upos_unique_words = merge_tags(partial_results)\n",
    "\n",
    "print('Unique words per universal tag:')\n",
    "print('\\n'.join([f' {k}:\\t{len(words)}' for k, words in upos_unique_words.items()]))\n",
    "            \n",
    "idx2upos = {idx: word for idx, word in enumerate(['eos', 'unk'] + list(upos_vocab.keys()))}\n",
    "upos2idx = {word: idx for idx, word in enumerate(['eos', 'unk'] + list(upos_vocab.keys()))}\n",
    "print(len(idx2upos))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate ground-truth references files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../results/MSVD_val_references.txt', 'w') as f:\n",
    "    for vidx, cap in zip(valid_vidxs, valid_corpus):\n",
    "        f.write('{}\\t{}\\n'.format(vidx, cap.lower()))\n",
    "        \n",
    "with open('../results/MSVD_test_references.txt', 'w') as f:\n",
    "    for vidx, cap in zip(test_vidxs, test_corpus):\n",
    "        f.write('{}\\t{}\\n'.format(vidx, cap.lower()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate corpus.pkl file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "train_corpus_widxs = [[word2idx[w] if w in vocab else UNK for w in nltk.word_tokenize(cap.lower())] + [EOS] for cap in train_corpus]\n",
    "valid_corpus_widxs = [[word2idx[w] if w in vocab else UNK for w in nltk.word_tokenize(cap.lower())] + [EOS] for cap in valid_corpus]\n",
    "test_corpus_widxs = [[word2idx[w] if w in vocab else UNK for w in nltk.word_tokenize(cap.lower())] + [EOS] for cap in test_corpus]\n",
    "\n",
    "train_corpus_pidxs = [[pos2idx[w[1]] if w[1] in pos_vocab else UNK for w in nltk.pos_tag(nltk.word_tokenize(cap.lower()))] + [EOS] for cap in train_corpus]\n",
    "valid_corpus_pidxs = [[pos2idx[w[1]] if w[1] in pos_vocab else UNK for w in nltk.pos_tag(nltk.word_tokenize(cap.lower()))] + [EOS] for cap in valid_corpus]\n",
    "test_corpus_pidxs = [[pos2idx[w[1]] if w[1] in pos_vocab else UNK for w in nltk.pos_tag(nltk.word_tokenize(cap.lower()))] + [EOS] for cap in test_corpus]\n",
    "\n",
    "assert len(train_corpus_widxs) == len(train_vidxs) and len(train_vidxs) == len(train_corpus_pidxs) and len(train_vidxs) == len(train_corpus), f'{len(train_vidxs)}, {len(train_corpus_widxs)}, {len(train_corpus_pidxs)}, {len(train_corpus)}'\n",
    "assert len(valid_corpus_widxs) == len(valid_vidxs) and len(valid_vidxs) == len(valid_corpus_pidxs) and len(valid_vidxs) == len(valid_corpus), f'{len(valid_vidxs)}, {len(valid_corpus_widxs)}, {len(valid_corpus_pidxs)}, {len(valid_corpus)}'\n",
    "assert len(test_corpus_widxs) == len(test_vidxs) and len(test_vidxs) == len(test_corpus_pidxs) and len(test_vidxs) == len(test_corpus), f'{len(test_vidxs)}, {len(test_corpus_widxs)}, {len(test_corpus_pidxs)}, {len(test_corpus)}'\n",
    "\n",
    "train_data = [train_corpus_widxs, train_vidxs, train_corpus_pidxs, train_corpus]\n",
    "valid_data = [valid_corpus_widxs, valid_vidxs, valid_corpus_pidxs, valid_corpus]\n",
    "test_data = [test_corpus_widxs, test_vidxs, test_corpus_pidxs, test_corpus]\n",
    "\n",
    "with open('../../../data/MSVD/my_msvd_corpus.pkl', 'wb') as outfile:\n",
    "    pickle.dump([train_data, valid_data, test_data, vocab, idx2word, word_embeddings, idx2pos], outfile)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
